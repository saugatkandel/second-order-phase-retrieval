{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses the rosenbrock function to compare the LM methods:\n",
    "1. the classical LM method available in the scipy package (using tensorflow to supply the full jacobian to scipy)\n",
    "2. the classical LM method implemented in this package\n",
    "3. using the LM method with the Generalized Gauss Newton matrix for a Newton-like approach, \n",
    "   with the *prediction function* is just the identity transform, and the entire rosenbrock function is inside the *loss function*\n",
    "   \n",
    "**Other notes**:\n",
    "- Since the scipy calculation uses the actual matrix inversion, we require a highly precise CG result with the matrix-free method for a valid comparison. \n",
    "- The scipy optimizer records the number of jacobian evaluations. Each full *iteration* of the matrix-free method can be thought of as an equivalent operation.\n",
    "- We want all the LM methods to have:\n",
    "    - identical output variable\n",
    "    - similar number of jacobian evaluations (or iterations).\n",
    "- We perform the comparisons with 10 different random initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sopt.optimizers.tensorflow2 import LMA, NonLinearConjugateGradient\n",
    "import scipy.optimize as spopt\n",
    "from tqdm.notebook import tqdm\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_rosenbrock(x):\n",
    "    # outputs the residuals\n",
    "    y1 = 10 * (x[1:] - x[:-1]**2)\n",
    "    y2 = 1 - x[:-1]\n",
    "    return tf.concat((y1, y2), axis=0)\n",
    "\n",
    "def fun_loss(x):\n",
    "    return 0.5 * tf.reduce_sum(x**2)\n",
    "\n",
    "def ggn_preds_fn(v):\n",
    "    return v\n",
    "def ggn_loss_fn(v):\n",
    "    return fun_loss(fun_rosenbrock(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_fun_rosenbrock(x):\n",
    "    t = tf.constant(x, dtype='float32')\n",
    "    out = fun_rosenbrock(t).numpy()\n",
    "    return out\n",
    "def scipy_jacobian_rosenbrock(x):\n",
    "    t = tf.constant(x, dtype='float32')\n",
    "    with tf.GradientTape() as gt:\n",
    "        gt.watch(t)\n",
    "        rsnbrck = fun_rosenbrock(t)\n",
    "    jac = gt.jacobian(rsnbrck, t)\n",
    "    return jac.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runScipyLM(init):\n",
    "    res = spopt.least_squares(scipy_fun_rosenbrock, init, jac=scipy_jacobian_rosenbrock, method='lm')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClassicLM(init, n_iterations, supply_diag_hessian=False):\n",
    "    results = {}\n",
    "    diag_hessian_fn = None\n",
    "    if supply_diag_hessian:\n",
    "        diag_hessian_fn = lambda v: 1.0\n",
    "    v = tf.Variable(init, dtype='float32')\n",
    "    lm = LMA(input_var=v, predictions_fn=fun_rosenbrock,\n",
    "             loss_fn=fun_loss, diag_hessian_fn=diag_hessian_fn,\n",
    "             name='lma', warm_start=True, \n",
    "             min_cg_tol=1e-5)\n",
    "    for j in range(n_iterations):\n",
    "        lm.minimize()\n",
    "    results['x'] = v.numpy()\n",
    "    results['cost'] = lm._loss_new.numpy()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGGNLM(init, n_iterations):\n",
    "    results = {}\n",
    "    v = tf.Variable(init, dtype='float32')\n",
    "    lm = LMA(input_var=v, predictions_fn=ggn_preds_fn,\n",
    "             loss_fn=ggn_loss_fn,\n",
    "             name='lma', warm_start=True, \n",
    "             min_cg_tol=1e-5)\n",
    "    for j in range(n_iterations):\n",
    "        lm.minimize()\n",
    "    results['x'] = v.numpy()\n",
    "    results['cost'] = lm._loss_new.numpy()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d9ae5df3d645a4ac4b9a1d541638bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scipy_results = []\n",
    "lm_classic_results = []\n",
    "lm_classic_with_supplied_hessian_results = [] # this should be identical to lm_classic_results\n",
    "lm_ggn_results = []\n",
    "for i in tqdm(range(10)):\n",
    "    z_init = np.random.random(5).astype('float32')\n",
    "    res = runScipyLM(z_init)\n",
    "    scipy_results.append(res)\n",
    "    lm_classic_results.append(runClassicLM(z_init, res.njev))\n",
    "    lm_classic_with_supplied_hessian_results.append(runClassicLM(z_init, res.njev, True))\n",
    "    lm_ggn_results.append(runGGNLM(z_init, res.njev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [1.00000001 1.00000006 1.00000001 1.00000002 0.99999999] iterations 11\n",
      "x [1.00000002 0.99999997 1.00000002 1.00000002 0.99999999] iterations 6\n",
      "x [0.99999998 1.00000005 1.00000001 0.99999998 1.00000001] iterations 14\n",
      "x [1.00000002 1.00000004 1.00000005 1.         0.99999999] iterations 12\n",
      "x [0.99999997 1.00000003 1.00000004 0.99999998 1.00000003] iterations 15\n",
      "x [1.00000004 1.00000001 1.00000001 0.99999998 1.        ] iterations 14\n",
      "x [1.         0.99999998 0.99999999 1.00000001 1.00000002] iterations 15\n",
      "x [1.00000002 1.00000001 1.00000002 1.00000002 1.00000001] iterations 14\n",
      "x [1.00000002 1.00000001 1.00000005 0.99999999 1.00000001] iterations 13\n",
      "x [0.99999999 1.00000001 1.00000001 1.00000003 1.        ] iterations 14\n"
     ]
    }
   ],
   "source": [
    "for r in scipy_results:\n",
    "    print('x', r.x, 'iterations', r.njev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [0.99898297 0.99796224 0.995917   0.9918258  0.9836622 ] cost 4.453459e-05\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n"
     ]
    }
   ],
   "source": [
    "for l in lm_classic_results:\n",
    "    print('x', l['x'], 'cost', l['cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [0.99898297 0.99796224 0.995917   0.9918258  0.9836622 ] cost 4.453459e-05\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n",
      "x [1. 1. 1. 1. 1.] cost 0.0\n"
     ]
    }
   ],
   "source": [
    "for l in lm_classic_with_supplied_hessian_results:\n",
    "    print('x', l['x'], 'cost', l['cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [4.8016688e-01 2.3040651e-01 5.7718989e-02 1.2542443e-02 1.5040496e-04] cost 1.3680493\n",
      "x [0.9977753  0.9953664  0.9900318  0.9774746  0.94599736] cost 0.005179659\n",
      "x [0.99369323 0.98739755 0.97488266 0.9502482  0.90268326] cost 0.0016579088\n",
      "x [0.98016894 0.96062785 0.9225483  0.85058093 0.7227015 ] cost 0.015182137\n",
      "x [0.99998474 0.9999649  0.9999118  0.99975204 0.99922055] cost 4.329831e-06\n",
      "x [0.99999994 0.9999999  0.99999976 0.9999996  0.99999917] cost 3.0198066e-13\n",
      "x [0.9975235  0.99504036 0.99007386 0.98018396 0.96068937] cost 0.00026147277\n",
      "x [-0.9606199   0.93296415  0.8754156   0.7681596   0.5888228 ] cost 1.9655604\n",
      "x [0.9992161 0.9984289 0.9968506 0.9936918 0.9873958] cost 2.6459566e-05\n",
      "x [-0.9511946   0.9148273   0.8413313   0.70765203  0.49441844] cost 1.9705827\n"
     ]
    }
   ],
   "source": [
    "for l in lm_ggn_results:\n",
    "    print('x', l['x'], 'cost', l['cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
